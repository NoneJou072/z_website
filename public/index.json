[{"content":"","date":"21 November 2023","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"21 November 2023","permalink":"/authors/haoran-zhou/","section":"Authors","summary":"","title":"Haoran Zhou"},{"content":" GCRL with Sub-goal Selection / Generation # 本文翻译与参考自: Goal-Conditioned Reinforcement Learning: Problems and Solutions\n由于难以实现长期目标，可以把目标分割成许多短期目标来帮助智能体到达长期目标。 在期望目标被替换后收集经验，就是将环境的目标分布从$f$改成$p_g$，目标函数变成 $$ J(\\pi)=\\mathbb{E}{\\textstyle{s{t+1}\\sim \\mathcal{T}(\\cdot\\mid s_t,a_t),\\atop a_{t} \\sim \\pi(\\cdot\\mid s_{t},g),g\\sim f}}\\left[\\sum_t \\gamma^tr(s_t,a_t,g)\\right] $$ $f$可以是一个规则，能够从过去的经验中选择$g$，或者是根据某种准则学习出的一个函数。\n下面介绍几种选择子目标的方法。\n1. 中间难度 Intermediate difficulty # 准确评估到达目标的难度有助于确定在智能体的能力范围内学习什么，一个合理的目标应该超出智能体的范围（why？）\nAutomatic goal generation for reinforcement learning agents. 根据奖励给难度打分，用来把目标标记成正/负采样，然后训练一个GAN模型来生成具有合理难度分数的目标。 Learning with amigo: Adversarially motivated intrinsic goals. 提出了一种 teacher-student 框架。teacher充当目标提供者，尝试提供难度递增的目标。teacher从student在提供的目标上的表现中学习，student从环境中得到的外部奖励和teacher中得到的内部奖励学习。 Automatic curriculum learning through value disagreement. 把Q函数认知的不确定性看成是实现目标难度的衡量，用于构建一个分布来采样目标。当不确定性过高时，目标应该处于策略的知识边界，以一个合理的难度来学习。 2. 探索意识 Exploration awareness # 由于稀疏奖励限制了策略的学习效率，我们应该提高策略的探索能力，尽可能覆盖没有遇到的目标和状态。 在GCRL，许多研究者们提议使用子目标采样来得到一个更好的探索能力。\nMaximum entropy gain exploration for long horizon multi-goal reinforcement learning. 提出采样使过去已实现目标的熵最大化的目标，以提高目标空间（goal space）的覆盖范围。（这一点和SAC的目的很像） State-covering self-supervised reinforcement learning. 训练一个自监督的生成式模型来生成行为目标，该目标在过去的经验中对已实现目标进行了近似均匀的倾斜，表现为最大化期望目标分布的熵。 Unsupervised controlthrough non-parametric discriminative rewards. 选择从不同的replay buffer中均匀地采样过去已实现的目标作为行为目标，这些储存的目标间的距离要尽可能地远。 Visual reinforcement learning with imagined goals. 选择从经验回放中均匀地采样过去已实现的目标，用这些历史目标拟合生成式模型来生成行为目标。 Exploration via hindsight goal generation 通过 Wasserstein Barycenter 问题，生成了一组事后目标以进行探索。把生成的目标作为隐性课程，可以有效地使用当前值函数来学习。 Dynamical distance learning for semi-supervised and unsupervised skill discovery 学习一个距离函数，并从replay buffer中选择距离初始状态最远的目标来提高探索效率。 3. 从经验中搜索 Searching from experience. # 历史经验包含导向实现某些目标的路径点，可以使人类或智能体获益。\nHindsight planner. 从一个采样的回合中选择 $k$ 个关键的中间路径点，并训练一个RNN网络，根据给定的起始和终止点来生成序列。再应用这些中间点作为期望目标的序列来与环境交互。 Search on the replay buffer: Bridging planning and reinforcement learning. 在 replay buffer 上建立图，把状态作为节点，从起始状态到目标状态的距离作为边的权重。该工作利用了二值奖励下的状态值可以近似成到目标的距离这一直觉，使用图搜索去寻找路径点序列来实现目标，并不断通过在路径点上学习的策略来采取动作。 4. Model-based planning 和 Learn from experts. # model-based RL 与 learning from demonstrations 不在笔者的考虑范围内，故不在此翻译。\nRelabeling in GCRL # 与子目标选择类似，relabeling也替换了初始的期望目标以提高学习效率。二者的区别是：\nRelabeling 关注于在训练前替换经验容器中的历史数据； Sub-goal Selection 关注于改变采样经验的分布。 为了实现重标记操作，理论上需要一个重标记函数 $h$ 去替换从 replay buffer 采样出的 transition 中的目标，并重新计算奖励 $$ (s_t, a_t, g_t, r_t) \\gets (s_t, a_t, h(·), r(s_t, a_t, h(·))) $$ 1. 事后回放 Hindsight relabeling. # 该方法可以追溯到最著名的 Hindsight experience replay(HER)，想法源自于人类从失败的经验中来学习。HER 使用replay buffer相同轨迹中已实现目标来替换期望目标，同时减轻稀疏奖励的问题。\nCurriculum-guided hindsight experience replay. 提出 CHER，使用课程重标记方法自适应地从失败经验中选择重标记目标。课程的评价标准是与期望目标的相似度和目标的多样性。\nGoal densitybased hindsight experience prioritization for multi-goal robot manipulation reinforcement learning. 为已实现目标学习了一个稠密模型，用于优先标记少见的目标。以实现在少见经验中探索来提高采样效率。\n2. Relabeling by learning. # Guided goal generation for hindsight multigoal reinforcement learning 从过去的经验选择重标记目标限制了重标记目标的多样性，为了减轻该问题并能够重标记未见过的目标，这篇文章通过隐式地建模策略表现和已实现目标之间的关系来训练一个条件生成式RNN网络，用于根据当前策略的平均返回值生成重标记目标。 3. 先见之明 Foresight. # 人类不仅可以从失败中学习，还可以基于当前状态对未来作出规划。\nMapgo: Modelassisted policy optimization for goal-oriented tasks. 学习一个动态模型用来生成用于重标记的虚拟轨迹。先见目标重标记可以防止标记局限于历史数据的同质目标，规划出当前策略可以实现的新目标。 ","date":"21 November 2023","permalink":"/posts/gcrl%E7%AC%94%E8%AE%B0/","section":"Posts","summary":"GCRL with Sub-goal Selection / Generation # 本文翻译与参考自: Goal-Conditioned Reinforcement Learning: Problems and Solutions","title":"关于 Goal-Conditioned RL 中子目标选择的问题"},{"content":"","date":"21 November 2023","permalink":"/","section":"海豚的博客","summary":"","title":"海豚的博客"},{"content":" 前言 # 最近花了一些阅读和复现 Relay Hindsight Experience Replay（RHER） 算法，这篇文章写的非常扎实，但作者开源的代码是基于 tensorflow 的 baselines 框架编写的。我在使用 pytorch 重写的过程中有了一些心得体会，在这分享出来。\n原文传送门： Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\nRHER 的 GITHUB 链接： https://github.com/kaixindelele/RHER\n我自己使用 pytorch 复现的 GITHUB 链接： https://github.com/NoneJou072/rl-notebook\n非常感谢作者的工作。\n复现过程 # issue-1 # 作者在 baselines 中的更新策略的方法是，按顺序分别训练 push 子任务和 reach 子任务。在 issue-5 中也提到了 每次更新当前的，以及下一个任务的策略函数。但在我的复现过程中发现，这样会出现 灾难性遗忘，当更新下一个任务的策略时，上一次被更新的任务会被遗忘掉。\n但是作者又在 issue 中回答到：\n不能存轨迹，而是每次交互后，先目标重标记，再把所有的子任务的数据，分别存到对应的经验池中。\n论文中也提到：\n对于三物体任务，需要更新先前的部署细节：1）使用基于PyTorch的双延迟DDPG（TD3）方法[47]，而不是基于TensorFlow的DDPG方法。\n对于基于PyTorch框架的多物体操作任务，在每收集一次轨迹后需要立即重新标记，并将多个子任务的transition，分别存储在不同的经验池中。在每次更新时，样本独立收集，然后组合成一个完整的批次。\n一个令我比较疑惑的点是，为什么三物体不接着延续基于 tensorflow 的 baselines 框架，为什么使用 baselines 框架时按顺序更新策略函数不会出现遗忘问题。\nreproduction-1 # 于是，我使用 pytorch 的 DDPG 方法，按照上述的描述进行了修改，并加入了一些我自己的调整，大概的伪代码为\n分别为 push 和 reach 子任务创建两个经验池 与环境交互，得到一个 episode 的 transitions 回合结束后，分割出当前 episode 属于 reach 子任务的 transitions。之后分别将轨迹存到对应的经验池中。 策略更新时，分别从两个经验池中采样，然后组合成一个完整的批次。 issue-2 # 使用基于 pytorch 编写的 DDPG 方法进行训练时，发现奖励函数的值会先抵达 0.8 左右的成功率，之后便会降低到 0.4 左右。\n在 DDPG 中，actor loss 表示负的 Q 值，actor loss 越小，Q 值越大。此时 actor loss 呈现出很大的下降趋势，critic loss 呈现很大的震荡。明显是出现 Q 值被高估的情况了。\n为什么会有Q值高估问题出现？-\u0026gt; 曾伊言: 强化学习算法TD3论文的翻译与解读。\n由于 Q 值过大，会导致时序差分目标 q_targets 过大\n参考自 Y. F. Zhang: 一个AC类算法策略loss引出的思考\n结果对比 # 下面是作者在 FetchPush-v1 环境下的结果：\n下面是我在 FetchPush-v2 环境下的复现结果：\n可以看到，在训练时间 56min 左右时，成功率稳定在了 95% 左右，与作者的结果相差不大。\n","date":"24 October 2023","permalink":"/posts/rher%E5%A4%8D%E7%8E%B0/","section":"Posts","summary":"前言 # 最近花了一些阅读和复现 Relay Hindsight Experience Replay（RHER） 算法，这篇文章写的非常扎实，但作者开源的代码是基于 tensorflow 的 baselines 框架编写的。我在使用 pytorch 重写的过程中有了一些心得体会，在这分享出来。","title":"Relayed HER复现结果与分析"},{"content":" 前言 # 意外发现 Mistral AI 开源的 Mistral 7B，声称在所有基准测试中的表现均优于 Llama2-13B，迫不及待的在公司服务器上进行了部署。测试后，发现效果真的很好：\n使用相同的 Prompt 测试代码生成能力，Mistral-7B 效果略次于 CodeLlama-13B-8bit 模型。Mistral-7B 及其 4、8bit 量化版本差异不大。 Mistral-7B 逻辑推理能力很差。如：a=1, b=2, n=a+b, n=?，Mistral-7B 无法回答。 推理速度很快，目测在1s内可以完成一次推理。 我想在自己的个人笔记本电脑上也进行部署，我的需求如下：\n部署在 Windows 11 系统上 使用显存大小为 8G 的 Geforce 4060 综上，只能选择 Mistral-7B 的 4bit 量化版本。但由于 bitsandbytes 不支持 windows，因此无法使用 bitsandbytes 进行量化。幸运的是，有人已经将 Mistral-7B 量化为 4bit，我只需要下载即可。\n模型下载 # 部署流程参考 CSDN博客\n首先，在 TheBloke/Mistral-7B-OpenOrca-GGUF 中下载推荐使用的 4bit 模型文件 mistral-7b-openorca.Q4_K_M.gguf, 该模型运行所需要的最大内存为 6.87 GB。\npip install ctransformers[cuda] ","date":"18 October 2023","permalink":"/posts/mistral-7b-%E9%83%A8%E7%BD%B2/","section":"Posts","summary":"前言 # 意外发现 Mistral AI 开源的 Mistral 7B，声称在所有基准测试中的表现均优于 Llama2-13B，迫不及待的在公司服务器上进行了部署。测试后，发现效果真的很好：","title":"Mistral-7B 部署"},{"content":" contype: int, “1” / conaffinity: int, “1” # contype 和 conaffinity 指定用于动态生成接触对的32位整数位掩码（请参见 Computation 章节中的 碰撞检测）。如果一个geom的contype与另一个geom的conaffinity 是 \u0026ldquo;compatible\u0026rdquo; 的，则两个geom可以发生碰撞。\n\u0026ldquo;compatible\u0026ldquo;意味着两个位掩码具有一个公共位设置为1。即： (contype1 \u0026amp; conaffinity2) || (contype2 \u0026amp; conaffinity1) 的结果布尔值为 True。\n由于默认值 contype = conaffinity = 1，因此平时可以忽略该参数的设置。\ncondim: int, “3” # 对于动态生成的接触对的接触空间的维度，设置为两个 geoms 中最大 condim 的值 （参考 Computation 章节中的 Contact）。\n下面是可用的值和对应的含义:\ncondim Description 1 Frictionless contact. 3 Regular frictional contact, opposing slip in the tangent plane. 4 Frictional contact, opposing slip in the tangent plane and rotation around the contact normal. This is useful for modeling soft contacts (independent of contact penetration). 6 Frictional contact, opposing slip in the tangent plane, rotation around the contact normal and rotation around the two axes of the tangent plane. The latter frictional effects are useful for preventing objects from indefinite rolling. ","date":"23 September 2023","permalink":"/posts/mujoco%E4%B8%ADgeom%E7%BB%84%E4%BB%B6%E7%9A%84%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/","section":"Posts","summary":"contype: int, “1” / conaffinity: int, “1” # contype 和 conaffinity 指定用于动态生成接触对的32位整数位掩码（请参见 Computation 章节中的 碰撞检测）。如果一个geom的contype与另一个geom的conaffinity 是 \u0026ldquo;compatible\u0026rdquo; 的，则两个geom可以发生碰撞。","title":"mujoco-geom组件参数说明"},{"content":" 介绍 # 使用三维软件建模后，发现导出的mesh文件是由三角形组成的面，那么如何将该 mesh 转成由四边形组成的面呢？可以使用 Blender 来完成这一操作。\n我们需要安装一个 Blender 插件： jremesh-tools，直接在 github 选择下载 ZIP 压缩包。\n参照提供的部署流程，我们再去下载 instant-meshes，可以直接下载编译好的二进制文件- Pre-compiled binaries(windows)。\n然后，将 jremesh-tools 插件导入到 blender 中：\n插件安装流程：Blender-编辑-偏好设置-插件-安装-选择下载好的zip压缩包-安装插件-勾选单选框 之后，点开单选框左边的箭头，在偏好设置中，将之前下载好的 instant-meshes 的二进制文件 Instant Meshes.exe 添加到路径中。\n添加完路径后，在 Blender 主界面按 N 键，出现选项卡，选择 JRemesh\n之后，导入并选中我们的 mesh 模型，点击重构网格，即可生成由四边形面片构成的 mesh。\n如果报错 JRemesh: Path to Instant Meshes is missing ，参考 JRemesh: Path to Instant Meshes is missing. #9，是 instant-meshes 的路径没有正确索引到。\n","date":"23 September 2023","permalink":"/posts/%E4%BD%BF%E7%94%A8blender%E5%B0%86%E4%B8%89%E8%A7%92%E5%BD%A2mesh%E8%BD%AC%E4%B8%BA%E5%9B%9B%E8%BE%B9%E5%BD%A2/","section":"Posts","summary":"介绍 # 使用三维软件建模后，发现导出的mesh文件是由三角形组成的面，那么如何将该 mesh 转成由四边形组成的面呢？可以使用 Blender 来完成这一操作。","title":"使用Blender将三角形mesh转为四边形"},{"content":" 1. 注册 PyPi 账户并验证 # 1.为了上传到 PyPi，我们需要在 pypi 网站中注册一个账户，注册完会给你的邮箱发送邮件，要点开邮件中的链接来验证身份。\n之后，会跳转到二次身份验证页面(2FA)。根据指引，我们生成了几行 Account recovery codes，记得先保存下来，再点击继续，把刚才保存的安全码中的一行输入进去，即可验证成功。\n接着，我们开始进行账户的二次验证。pypi 提供了两个途径，分别是 Add 2FA with authentication application 和 Add 2FA with security device。\n我们选择 with authentication application 的方式。去谷歌商店中下载 Microsoft Authenticator，打开 app 后，选择底部的已验证ID-扫描QR码，扫描 Pypi 网页中出现的 QR 码。之后切换到 Authenticator 界面，点开 PyPi，会出现一次性密码代码，将该代码输入到 PyPi 网页中 QR 码右侧的输入栏，然后点击验证即可。\n2. 调整本地项目的文件结构 # 要打包的项目应该符合如下文件结构：\n/packaging_tutorial /example_pkg __init__.py setup.py LICENSE README.md 其中，\nsetup.py 是setuptools的构建脚本。它告诉setuptools你的包（例如名称和版本）以及要包含的代码文件。 LICENSE 文件中包含着我们的许可证，用于告诉用户安装你的软件包可以使用您的软件包的条款。有关选择许可证的帮助，请访问 https://choosealicense.com/。选择许可证后，可以将其内容拷贝到 LICENSE 文本文件中。 README.md 可以放入一些对项目的介绍。 3. 配置 setup.py # setup.py是 setuptools 的构建脚本。它告诉 setuptools 你的包（例如名称和版本）以及要包含的代码文件。下面是一个setup.py内容示例：\nimport setuptools with open(\u0026#34;README.md\u0026#34;, \u0026#34;r\u0026#34;) as fh: long_description = fh.read() setuptools.setup( name=\u0026#34;example-pkg-your-username\u0026#34;, version=\u0026#34;0.0.1\u0026#34;, author=\u0026#34;Example Author\u0026#34;, author_email=\u0026#34;author@example.com\u0026#34;, description=\u0026#34;A small example package\u0026#34;, long_description=long_description, long_description_content_type=\u0026#34;text/markdown\u0026#34;, url=\u0026#34;https://github.com/pypa/sampleproject\u0026#34;, packages=setuptools.find_packages(), classifiers=[ \u0026#34;Programming Language :: Python :: 3\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Operating System :: OS Independent\u0026#34;, ], ) 对于setup()中参数的介绍如下：\nname - 包的分发名称。只要包含字母，数字_和，就可以是任何名称-。它也不能在pypi.org上使用。请务必使用您的用户名更新此内容，因为这可确保您在上传程序包时不会遇到任何名称冲突。 version - 包的版本。 author - 包的作者。 author_email 包的作者邮箱。 description - 包的一个简短的总结。 long_description - 包的详细说明。这显示在Python Package Index的包详细信息包中。在这种情况下，加载长描述README.md是一种常见模式。 long_description_content_type - 告诉索引什么类型的标记用于长描述。在这种情况下，它是Markdown。 url - 是项目主页的URL。在许多项目中是一个指向GitHub，GitLab，Bitbucket或类似代码托管服务的链接。 packages - 应包含在分发包中的所有导入包的列表。我们可以使用 find_packages() 自动发现所有包和子包。在示例中，包列表为 example_pkg，因为它是唯一存在的包。 classifiers - 关于包的其他元数据。在示例中，该软件包仅与 Python 3 兼容，根据MIT许可证进行许可，且与操作系统无关。有关分类器的完整列表，参阅 https://pypi.org/classifiers/。 除了这里提到的还有很多。有关详细信息可以参阅 打包和分发项目。\n4. 使用 MANIFEST.in 打包静态资源 # 在项目目录下新建 MANIFEST.in 文件，里面的内容参考如下示例：\ninclude *.txt #包含根目录下的所有txt文件 recursive-include examples *.txt *.py #包含所有位置的examples文件夹下的txt与py文件 prune examples/sample?/build #排除所有位置examples/sample?/build global-include * # 包含安装包下的所有文件 5. 生成分发档案 # 下一步是为包生成分发包。这些是上传到包索引的档案，可以通过pip安装。\n首先确保安装了最新版本的 setuptools 和 wheel ：\npython3 -m pip install --user --upgrade setuptools wheel 现在从与setup.py位于的同一目录运行此命令：\npython3 setup.py sdist bdist_wheel 完成后，会在生成 dist 文件夹，并包含两个文件：\ndist/ example_pkg_your_username-0.0.1-py3-none-any.whl example_pkg_your_username-0.0.1.tar.gz 6. 上传 PYPI # 接下来我们开始将项目包上传到 PYPI，首先进入 PyPi 账户设置中的 Create API token 界面，根据指引创建新的 API token，记得将生成的 token 复制下来。然后继续按照教程配置 token，具体地：\n新建或修改$HOME/.pypirc文件 (windows 中在 users`` 目录下新建 .pypirc`) 文件内容的模板如下：\n[pypi] username = __token__ password = pypi-AgEIcHlwaS5vcmcCJGYzYjQzM2NjLWFiYTMtNGM1Zi05ZWU1LTQ2MTUzOGUyNzA3NwACKlszLCJmMzkyMGY4YS05ZTVkLTQyMmEtOGE2MS1lM2IyYjJhZTNlMjgiXQAABiDQYlxiUUsdywZ2RtK__n0x0lgobwbai2ocPdFTRVG_ig 回到项目命令行，输入\nsudo apt install twine twine upload dist/* 输入 PyPI 的用户名和密码。命令完成后，应该看到与此类似的输出：\nUploading distributions to https://test.pypi.org/legacy/ Enter your username: [your username] Enter your password: Uploading example_pkg_your_username-0.0.1-py3-none-any.whl 100%|█████████████████████| 4.65k/4.65k [00:01\u0026lt;00:00, 2.88kB/s] Uploading example_pkg_your_username-0.0.1.tar.gz 100%|█████████████████████| 4.25k/4.25k [00:01\u0026lt;00:00, 3.05kB/s] 现在，我们的包已经上传到 PyPi 上了，可以进入到 projects 界面查看我们的项目。\n","date":"7 September 2023","permalink":"/posts/pypi/","section":"Posts","summary":"1.","title":"打包自己的 Python 包并上传 PyPi"},{"content":"","date":"29 January 2023","permalink":"/authors/harlan-chou/","section":"Authors","summary":"","title":"Harlan-Chou"},{"content":" Marching Cubes 详解与实现 # 等值面是空间中的一张曲面，在该曲面上函数F(x，y，z)的值等于某一给定值。准确地讲，是指在某一网格空间中，假若每一结点保存着三变量函数F(x，y，z)，而且网格单元在x，y，z方向上的连续采样值为F(x，y，z)，则对于某一给定值Fi，等值面是由所有满足\n$S={(x，y，z) | F(x，y，z)=Fi}$\n的点组成的一张曲面。\n按照此严格定义下得到的等值面表达式如下：\n$F(x，y，z)=a_0+a_1x+a_2y+a_3z+a_4xy+a_5yz+a_6xz+a_7xyz$\nMarching Cubes（MC）算法是一种三维重建算法，常被用于三维重建任务，也在一些游戏中用于程序生成地形。简单来说，对于某个三维物体，例如下图所示的蓝色物体，由于直接提取该物体的等值面较为繁琐，MC算法意图使用近似的等值面对该物体重新建模，以简化等值面的提取。\n1. Cube Index # 首先，我们需要将该物体细分为一个个单元格（grid），每个单元格的各个顶点和边都用一个标量来索引。\n​ ​\n接下来，我们要判断各单元格的各顶点是否在3D几何体内。在覆盖物体外围的单元格中，物体的表面会与单元格相交，将单元格的某些顶点分隔在外面，我们用 cubeindex 来表示 8 个顶点的内外情况。\ncubeindex 包含 8 个 bit，分别表示单元格的 8 个顶点。假设第一个顶点在等值面的外面，我们令 cubeindex 与 1 作与运算，则 cubeindex 的第 1 个 bit 被置为 1。同理，若第二个顶点在等值面外，我们令 cubeindex 与 2 作与运算，2 的二进制表示为 10，则 cubeindex 的第 2 个 bit 被置为 1。\ncubeindex = 0; if (grid.val[0] \u0026lt; isolevel) cubeindex |= 1; if (grid.val[1] \u0026lt; isolevel) cubeindex |= 2; if (grid.val[2] \u0026lt; isolevel) cubeindex |= 4; if (grid.val[3] \u0026lt; isolevel) cubeindex |= 8; if (grid.val[4] \u0026lt; isolevel) cubeindex |= 16; if (grid.val[5] \u0026lt; isolevel) cubeindex |= 32; if (grid.val[6] \u0026lt; isolevel) cubeindex |= 64; if (grid.val[7] \u0026lt; isolevel) cubeindex |= 128; 其中，isoLevel​表示等值面（isolevel surface）的阈值。Marching Cubes是一种用于从三维体数据生成等值面的算法，等值面是指具有相同数值的体数据点构成的表面。isoLevel​定义了等值面所代表的数值。在算法中，如果一个体数据点的值低于isoLevel​，则该点被认为在等值面的外面；反之，如果一个体数据点的值高于或等于isoLevel​，则该点被认为在等值面内部。\n现在，我们以 Marching Square 为例，尝试重建一个圆出来。本例参考自[2]，图例为本人绘制。\n首先设定一些参数，圆的半径radius​，圆心在世界中的实际坐标center​。我们使用7*7的网格来进行重建，其中各轴上点的数量numPointPerAxis​为 8，那么各轴上 cube 的数量numVoxelPerAxis​为numPointPerAxis-1=7​个，id​ 表示各点在网格中的的索引坐标。另外设定每个 cube 的实际长度为cubeSize​。在本例中，我们将网格的中心与圆心对齐，这在后面的偏置计算中会用到。\n​ ​\n各顶点在网格中的实际坐标为 id * cubeSize​ ，网格中心(圆心)的实际坐标为 numPointsPerAxis * cubeSize * 0.5​ ，则该顶点相对圆心的偏置为：\nfloat3 offset = id * cubeSize - numPointsPerAxis * cubeSize * 0.5; 最终，我们可以得到这个点的位置和这个点对应的值。\nfloat3 pos = center + offset; float value = length(offset) - radius 最后，我们拿这个值与等值面的阈值 isolevel 进行比较，判断这个点在等值面内还是面外，并为相应的位值1或0。\n2. Cube Edge # 接下来，我们以上述运算完得到的 cubeindex 的十进制作为索引，找出在 edgeTable 中对应的一个十六进制数。这个十六进制数表示了单元格中 12 条边的被切割情况。\nint edgeTable[256]={ 0x0 , 0x109, 0x203, 0x30a, 0x406, 0x50f, 0x605, 0x70c, 0x80c, 0x905, 0xa0f, 0xb06, 0xc0a, 0xd03, 0xe09, 0xf00, 0x190, 0x99 , 0x393, 0x29a, 0x596, 0x49f, 0x795, 0x69c, 0x99c, 0x895, 0xb9f, 0xa96, 0xd9a, 0xc93, 0xf99, 0xe90, 0x230, 0x339, 0x33 , 0x13a, 0x636, 0x73f, 0x435, 0x53c, 0xa3c, 0xb35, 0x83f, 0x936, 0xe3a, 0xf33, 0xc39, 0xd30, 0x3a0, 0x2a9, 0x1a3, 0xaa , 0x7a6, 0x6af, 0x5a5, 0x4ac, 0xbac, 0xaa5, 0x9af, 0x8a6, 0xfaa, 0xea3, 0xda9, 0xca0, 0x460, 0x569, 0x663, 0x76a, 0x66 , 0x16f, 0x265, 0x36c, 0xc6c, 0xd65, 0xe6f, 0xf66, 0x86a, 0x963, 0xa69, 0xb60, 0x5f0, 0x4f9, 0x7f3, 0x6fa, 0x1f6, 0xff , 0x3f5, 0x2fc, 0xdfc, 0xcf5, 0xfff, 0xef6, 0x9fa, 0x8f3, 0xbf9, 0xaf0, 0x650, 0x759, 0x453, 0x55a, 0x256, 0x35f, 0x55 , 0x15c, 0xe5c, 0xf55, 0xc5f, 0xd56, 0xa5a, 0xb53, 0x859, 0x950, 0x7c0, 0x6c9, 0x5c3, 0x4ca, 0x3c6, 0x2cf, 0x1c5, 0xcc , 0xfcc, 0xec5, 0xdcf, 0xcc6, 0xbca, 0xac3, 0x9c9, 0x8c0, 0x8c0, 0x9c9, 0xac3, 0xbca, 0xcc6, 0xdcf, 0xec5, 0xfcc, 0xcc , 0x1c5, 0x2cf, 0x3c6, 0x4ca, 0x5c3, 0x6c9, 0x7c0, 0x950, 0x859, 0xb53, 0xa5a, 0xd56, 0xc5f, 0xf55, 0xe5c, 0x15c, 0x55 , 0x35f, 0x256, 0x55a, 0x453, 0x759, 0x650, 0xaf0, 0xbf9, 0x8f3, 0x9fa, 0xef6, 0xfff, 0xcf5, 0xdfc, 0x2fc, 0x3f5, 0xff , 0x1f6, 0x6fa, 0x7f3, 0x4f9, 0x5f0, 0xb60, 0xa69, 0x963, 0x86a, 0xf66, 0xe6f, 0xd65, 0xc6c, 0x36c, 0x265, 0x16f, 0x66 , 0x76a, 0x663, 0x569, 0x460, 0xca0, 0xda9, 0xea3, 0xfaa, 0x8a6, 0x9af, 0xaa5, 0xbac, 0x4ac, 0x5a5, 0x6af, 0x7a6, 0xaa , 0x1a3, 0x2a9, 0x3a0, 0xd30, 0xc39, 0xf33, 0xe3a, 0x936, 0x83f, 0xb35, 0xa3c, 0x53c, 0x435, 0x73f, 0x636, 0x13a, 0x33 , 0x339, 0x230, 0xe90, 0xf99, 0xc93, 0xd9a, 0xa96, 0xb9f, 0x895, 0x99c, 0x69c, 0x795, 0x49f, 0x596, 0x29a, 0x393, 0x99 , 0x190, 0xf00, 0xe09, 0xd03, 0xc0a, 0xb06, 0xa0f, 0x905, 0x80c, 0x70c, 0x605, 0x50f, 0x406, 0x30a, 0x203, 0x109, 0x0 }; 在如下面的左图所示，在一个单元格中，假设只有点 3 在等值面外，我们容易得到 cubeindex = 0000 1000 ，对应的十进制为 8。查找表 edgeTable 得到 edgeTable[8] = 1000 0000 1100，表示边 2,3, and 11 与等值面相交（如右图所示）。\n​ ​\n接下来，我们通过插值的方法找出来三个相交点的坐标 P：\n$P = P_1 + (isovalue - V_1) (P_2 - P_1) / (V_2 - V_1)$\nP1 和 P2 是相交边的两个顶点的坐标，如边2上的点 2 和 3。V1、V2 是这两个点在该 cube 中的值。\n另外，有人建议插值应按[3]所示处理，这解决了一个等值面上小裂缝的问题。\n3. Triangular Table # 最后，我们得到了 n 个坐标点，这些坐标点可以组成 m 个三角形，这就需要我们按照一定的顺序来连接各个顶点，可以通过查表 triTable 来获得。\nint triTable[256][16] = {{-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 8, 3, 9, 8, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 2, 10, 0, 2, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 8, 3, 2, 10, 8, 10, 9, 8, -1, -1, -1, -1, -1, -1, -1}, {3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 11, 2, 8, 11, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 11, 2, 1, 9, 11, 9, 8, 11, -1, -1, -1, -1, -1, -1, -1}, {3, 10, 1, 11, 10, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 10, 1, 0, 8, 10, 8, 11, 10, -1, -1, -1, -1, -1, -1, -1}, {3, 9, 0, 3, 11, 9, 11, 10, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 3, 0, 7, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 1, 9, 4, 7, 1, 7, 3, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 4, 7, 3, 0, 4, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1}, {9, 2, 10, 9, 0, 2, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1}, {2, 10, 9, 2, 9, 7, 2, 7, 3, 7, 9, 4, -1, -1, -1, -1}, {8, 4, 7, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 4, 7, 11, 2, 4, 2, 0, 4, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 1, 8, 4, 7, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1}, {4, 7, 11, 9, 4, 11, 9, 11, 2, 9, 2, 1, -1, -1, -1, -1}, {3, 10, 1, 3, 11, 10, 7, 8, 4, -1, -1, -1, -1, -1, -1, -1}, {1, 11, 10, 1, 4, 11, 1, 0, 4, 7, 11, 4, -1, -1, -1, -1}, {4, 7, 8, 9, 0, 11, 9, 11, 10, 11, 0, 3, -1, -1, -1, -1}, {4, 7, 11, 4, 11, 9, 9, 11, 10, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 5, 4, 1, 5, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 5, 4, 8, 3, 5, 3, 1, 5, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 1, 2, 10, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1}, {5, 2, 10, 5, 4, 2, 4, 0, 2, -1, -1, -1, -1, -1, -1, -1}, {2, 10, 5, 3, 2, 5, 3, 5, 4, 3, 4, 8, -1, -1, -1, -1}, {9, 5, 4, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 11, 2, 0, 8, 11, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1}, {0, 5, 4, 0, 1, 5, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1}, {2, 1, 5, 2, 5, 8, 2, 8, 11, 4, 8, 5, -1, -1, -1, -1}, {10, 3, 11, 10, 1, 3, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 5, 0, 8, 1, 8, 10, 1, 8, 11, 10, -1, -1, -1, -1}, {5, 4, 0, 5, 0, 11, 5, 11, 10, 11, 0, 3, -1, -1, -1, -1}, {5, 4, 8, 5, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1}, {9, 7, 8, 5, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 3, 0, 9, 5, 3, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 7, 8, 0, 1, 7, 1, 5, 7, -1, -1, -1, -1, -1, -1, -1}, {1, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 7, 8, 9, 5, 7, 10, 1, 2, -1, -1, -1, -1, -1, -1, -1}, {10, 1, 2, 9, 5, 0, 5, 3, 0, 5, 7, 3, -1, -1, -1, -1}, {8, 0, 2, 8, 2, 5, 8, 5, 7, 10, 5, 2, -1, -1, -1, -1}, {2, 10, 5, 2, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1}, {7, 9, 5, 7, 8, 9, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 7, 9, 7, 2, 9, 2, 0, 2, 7, 11, -1, -1, -1, -1}, {2, 3, 11, 0, 1, 8, 1, 7, 8, 1, 5, 7, -1, -1, -1, -1}, {11, 2, 1, 11, 1, 7, 7, 1, 5, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 8, 8, 5, 7, 10, 1, 3, 10, 3, 11, -1, -1, -1, -1}, {5, 7, 0, 5, 0, 9, 7, 11, 0, 1, 0, 10, 11, 10, 0, -1}, {11, 10, 0, 11, 0, 3, 10, 5, 0, 8, 0, 7, 5, 7, 0, -1}, {11, 10, 5, 7, 11, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 1, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 8, 3, 1, 9, 8, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 5, 2, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 5, 1, 2, 6, 3, 0, 8, -1, -1, -1, -1, -1, -1, -1}, {9, 6, 5, 9, 0, 6, 0, 2, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 9, 8, 5, 8, 2, 5, 2, 6, 3, 2, 8, -1, -1, -1, -1}, {2, 3, 11, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 0, 8, 11, 2, 0, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 1, 9, 2, 9, 11, 2, 9, 8, 11, -1, -1, -1, -1}, {6, 3, 11, 6, 5, 3, 5, 1, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 11, 0, 11, 5, 0, 5, 1, 5, 11, 6, -1, -1, -1, -1}, {3, 11, 6, 0, 3, 6, 0, 6, 5, 0, 5, 9, -1, -1, -1, -1}, {6, 5, 9, 6, 9, 11, 11, 9, 8, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 3, 0, 4, 7, 3, 6, 5, 10, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 5, 10, 6, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1}, {10, 6, 5, 1, 9, 7, 1, 7, 3, 7, 9, 4, -1, -1, -1, -1}, {6, 1, 2, 6, 5, 1, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 5, 5, 2, 6, 3, 0, 4, 3, 4, 7, -1, -1, -1, -1}, {8, 4, 7, 9, 0, 5, 0, 6, 5, 0, 2, 6, -1, -1, -1, -1}, {7, 3, 9, 7, 9, 4, 3, 2, 9, 5, 9, 6, 2, 6, 9, -1}, {3, 11, 2, 7, 8, 4, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 4, 7, 2, 4, 2, 0, 2, 7, 11, -1, -1, -1, -1}, {0, 1, 9, 4, 7, 8, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1}, {9, 2, 1, 9, 11, 2, 9, 4, 11, 7, 11, 4, 5, 10, 6, -1}, {8, 4, 7, 3, 11, 5, 3, 5, 1, 5, 11, 6, -1, -1, -1, -1}, {5, 1, 11, 5, 11, 6, 1, 0, 11, 7, 11, 4, 0, 4, 11, -1}, {0, 5, 9, 0, 6, 5, 0, 3, 6, 11, 6, 3, 8, 4, 7, -1}, {6, 5, 9, 6, 9, 11, 4, 7, 9, 7, 11, 9, -1, -1, -1, -1}, {10, 4, 9, 6, 4, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 10, 6, 4, 9, 10, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1}, {10, 0, 1, 10, 6, 0, 6, 4, 0, -1, -1, -1, -1, -1, -1, -1}, {8, 3, 1, 8, 1, 6, 8, 6, 4, 6, 1, 10, -1, -1, -1, -1}, {1, 4, 9, 1, 2, 4, 2, 6, 4, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 1, 2, 9, 2, 4, 9, 2, 6, 4, -1, -1, -1, -1}, {0, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 3, 2, 8, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1}, {10, 4, 9, 10, 6, 4, 11, 2, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 2, 2, 8, 11, 4, 9, 10, 4, 10, 6, -1, -1, -1, -1}, {3, 11, 2, 0, 1, 6, 0, 6, 4, 6, 1, 10, -1, -1, -1, -1}, {6, 4, 1, 6, 1, 10, 4, 8, 1, 2, 1, 11, 8, 11, 1, -1}, {9, 6, 4, 9, 3, 6, 9, 1, 3, 11, 6, 3, -1, -1, -1, -1}, {8, 11, 1, 8, 1, 0, 11, 6, 1, 9, 1, 4, 6, 4, 1, -1}, {3, 11, 6, 3, 6, 0, 0, 6, 4, -1, -1, -1, -1, -1, -1, -1}, {6, 4, 8, 11, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 10, 6, 7, 8, 10, 8, 9, 10, -1, -1, -1, -1, -1, -1, -1}, {0, 7, 3, 0, 10, 7, 0, 9, 10, 6, 7, 10, -1, -1, -1, -1}, {10, 6, 7, 1, 10, 7, 1, 7, 8, 1, 8, 0, -1, -1, -1, -1}, {10, 6, 7, 10, 7, 1, 1, 7, 3, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 6, 1, 6, 8, 1, 8, 9, 8, 6, 7, -1, -1, -1, -1}, {2, 6, 9, 2, 9, 1, 6, 7, 9, 0, 9, 3, 7, 3, 9, -1}, {7, 8, 0, 7, 0, 6, 6, 0, 2, -1, -1, -1, -1, -1, -1, -1}, {7, 3, 2, 6, 7, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 11, 10, 6, 8, 10, 8, 9, 8, 6, 7, -1, -1, -1, -1}, {2, 0, 7, 2, 7, 11, 0, 9, 7, 6, 7, 10, 9, 10, 7, -1}, {1, 8, 0, 1, 7, 8, 1, 10, 7, 6, 7, 10, 2, 3, 11, -1}, {11, 2, 1, 11, 1, 7, 10, 6, 1, 6, 7, 1, -1, -1, -1, -1}, {8, 9, 6, 8, 6, 7, 9, 1, 6, 11, 6, 3, 1, 3, 6, -1}, {0, 9, 1, 11, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 8, 0, 7, 0, 6, 3, 11, 0, 11, 6, 0, -1, -1, -1, -1}, {7, 11, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 1, 9, 8, 3, 1, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1}, {10, 1, 2, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 3, 0, 8, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1}, {2, 9, 0, 2, 10, 9, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1}, {6, 11, 7, 2, 10, 3, 10, 8, 3, 10, 9, 8, -1, -1, -1, -1}, {7, 2, 3, 6, 2, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 0, 8, 7, 6, 0, 6, 2, 0, -1, -1, -1, -1, -1, -1, -1}, {2, 7, 6, 2, 3, 7, 0, 1, 9, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 2, 1, 8, 6, 1, 9, 8, 8, 7, 6, -1, -1, -1, -1}, {10, 7, 6, 10, 1, 7, 1, 3, 7, -1, -1, -1, -1, -1, -1, -1}, {10, 7, 6, 1, 7, 10, 1, 8, 7, 1, 0, 8, -1, -1, -1, -1}, {0, 3, 7, 0, 7, 10, 0, 10, 9, 6, 10, 7, -1, -1, -1, -1}, {7, 6, 10, 7, 10, 8, 8, 10, 9, -1, -1, -1, -1, -1, -1, -1}, {6, 8, 4, 11, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 6, 11, 3, 0, 6, 0, 4, 6, -1, -1, -1, -1, -1, -1, -1}, {8, 6, 11, 8, 4, 6, 9, 0, 1, -1, -1, -1, -1, -1, -1, -1}, {9, 4, 6, 9, 6, 3, 9, 3, 1, 11, 3, 6, -1, -1, -1, -1}, {6, 8, 4, 6, 11, 8, 2, 10, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 3, 0, 11, 0, 6, 11, 0, 4, 6, -1, -1, -1, -1}, {4, 11, 8, 4, 6, 11, 0, 2, 9, 2, 10, 9, -1, -1, -1, -1}, {10, 9, 3, 10, 3, 2, 9, 4, 3, 11, 3, 6, 4, 6, 3, -1}, {8, 2, 3, 8, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1}, {0, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 2, 3, 4, 2, 4, 6, 4, 3, 8, -1, -1, -1, -1}, {1, 9, 4, 1, 4, 2, 2, 4, 6, -1, -1, -1, -1, -1, -1, -1}, {8, 1, 3, 8, 6, 1, 8, 4, 6, 6, 10, 1, -1, -1, -1, -1}, {10, 1, 0, 10, 0, 6, 6, 0, 4, -1, -1, -1, -1, -1, -1, -1}, {4, 6, 3, 4, 3, 8, 6, 10, 3, 0, 3, 9, 10, 9, 3, -1}, {10, 9, 4, 6, 10, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 5, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 4, 9, 5, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 0, 1, 5, 4, 0, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1}, {11, 7, 6, 8, 3, 4, 3, 5, 4, 3, 1, 5, -1, -1, -1, -1}, {9, 5, 4, 10, 1, 2, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1}, {6, 11, 7, 1, 2, 10, 0, 8, 3, 4, 9, 5, -1, -1, -1, -1}, {7, 6, 11, 5, 4, 10, 4, 2, 10, 4, 0, 2, -1, -1, -1, -1}, {3, 4, 8, 3, 5, 4, 3, 2, 5, 10, 5, 2, 11, 7, 6, -1}, {7, 2, 3, 7, 6, 2, 5, 4, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, 0, 8, 6, 0, 6, 2, 6, 8, 7, -1, -1, -1, -1}, {3, 6, 2, 3, 7, 6, 1, 5, 0, 5, 4, 0, -1, -1, -1, -1}, {6, 2, 8, 6, 8, 7, 2, 1, 8, 4, 8, 5, 1, 5, 8, -1}, {9, 5, 4, 10, 1, 6, 1, 7, 6, 1, 3, 7, -1, -1, -1, -1}, {1, 6, 10, 1, 7, 6, 1, 0, 7, 8, 7, 0, 9, 5, 4, -1}, {4, 0, 10, 4, 10, 5, 0, 3, 10, 6, 10, 7, 3, 7, 10, -1}, {7, 6, 10, 7, 10, 8, 5, 4, 10, 4, 8, 10, -1, -1, -1, -1}, {6, 9, 5, 6, 11, 9, 11, 8, 9, -1, -1, -1, -1, -1, -1, -1}, {3, 6, 11, 0, 6, 3, 0, 5, 6, 0, 9, 5, -1, -1, -1, -1}, {0, 11, 8, 0, 5, 11, 0, 1, 5, 5, 6, 11, -1, -1, -1, -1}, {6, 11, 3, 6, 3, 5, 5, 3, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 9, 5, 11, 9, 11, 8, 11, 5, 6, -1, -1, -1, -1}, {0, 11, 3, 0, 6, 11, 0, 9, 6, 5, 6, 9, 1, 2, 10, -1}, {11, 8, 5, 11, 5, 6, 8, 0, 5, 10, 5, 2, 0, 2, 5, -1}, {6, 11, 3, 6, 3, 5, 2, 10, 3, 10, 5, 3, -1, -1, -1, -1}, {5, 8, 9, 5, 2, 8, 5, 6, 2, 3, 8, 2, -1, -1, -1, -1}, {9, 5, 6, 9, 6, 0, 0, 6, 2, -1, -1, -1, -1, -1, -1, -1}, {1, 5, 8, 1, 8, 0, 5, 6, 8, 3, 8, 2, 6, 2, 8, -1}, {1, 5, 6, 2, 1, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 3, 6, 1, 6, 10, 3, 8, 6, 5, 6, 9, 8, 9, 6, -1}, {10, 1, 0, 10, 0, 6, 9, 5, 0, 5, 6, 0, -1, -1, -1, -1}, {0, 3, 8, 5, 6, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {10, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 5, 10, 7, 5, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 5, 10, 11, 7, 5, 8, 3, 0, -1, -1, -1, -1, -1, -1, -1}, {5, 11, 7, 5, 10, 11, 1, 9, 0, -1, -1, -1, -1, -1, -1, -1}, {10, 7, 5, 10, 11, 7, 9, 8, 1, 8, 3, 1, -1, -1, -1, -1}, {11, 1, 2, 11, 7, 1, 7, 5, 1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 1, 2, 7, 1, 7, 5, 7, 2, 11, -1, -1, -1, -1}, {9, 7, 5, 9, 2, 7, 9, 0, 2, 2, 11, 7, -1, -1, -1, -1}, {7, 5, 2, 7, 2, 11, 5, 9, 2, 3, 2, 8, 9, 8, 2, -1}, {2, 5, 10, 2, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1}, {8, 2, 0, 8, 5, 2, 8, 7, 5, 10, 2, 5, -1, -1, -1, -1}, {9, 0, 1, 5, 10, 3, 5, 3, 7, 3, 10, 2, -1, -1, -1, -1}, {9, 8, 2, 9, 2, 1, 8, 7, 2, 10, 2, 5, 7, 5, 2, -1}, {1, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 7, 0, 7, 1, 1, 7, 5, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 3, 9, 3, 5, 5, 3, 7, -1, -1, -1, -1, -1, -1, -1}, {9, 8, 7, 5, 9, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {5, 8, 4, 5, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1}, {5, 0, 4, 5, 11, 0, 5, 10, 11, 11, 3, 0, -1, -1, -1, -1}, {0, 1, 9, 8, 4, 10, 8, 10, 11, 10, 4, 5, -1, -1, -1, -1}, {10, 11, 4, 10, 4, 5, 11, 3, 4, 9, 4, 1, 3, 1, 4, -1}, {2, 5, 1, 2, 8, 5, 2, 11, 8, 4, 5, 8, -1, -1, -1, -1}, {0, 4, 11, 0, 11, 3, 4, 5, 11, 2, 11, 1, 5, 1, 11, -1}, {0, 2, 5, 0, 5, 9, 2, 11, 5, 4, 5, 8, 11, 8, 5, -1}, {9, 4, 5, 2, 11, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 5, 10, 3, 5, 2, 3, 4, 5, 3, 8, 4, -1, -1, -1, -1}, {5, 10, 2, 5, 2, 4, 4, 2, 0, -1, -1, -1, -1, -1, -1, -1}, {3, 10, 2, 3, 5, 10, 3, 8, 5, 4, 5, 8, 0, 1, 9, -1}, {5, 10, 2, 5, 2, 4, 1, 9, 2, 9, 4, 2, -1, -1, -1, -1}, {8, 4, 5, 8, 5, 3, 3, 5, 1, -1, -1, -1, -1, -1, -1, -1}, {0, 4, 5, 1, 0, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 4, 5, 8, 5, 3, 9, 0, 5, 0, 3, 5, -1, -1, -1, -1}, {9, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 11, 7, 4, 9, 11, 9, 10, 11, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 4, 9, 7, 9, 11, 7, 9, 10, 11, -1, -1, -1, -1}, {1, 10, 11, 1, 11, 4, 1, 4, 0, 7, 4, 11, -1, -1, -1, -1}, {3, 1, 4, 3, 4, 8, 1, 10, 4, 7, 4, 11, 10, 11, 4, -1}, {4, 11, 7, 9, 11, 4, 9, 2, 11, 9, 1, 2, -1, -1, -1, -1}, {9, 7, 4, 9, 11, 7, 9, 1, 11, 2, 11, 1, 0, 8, 3, -1}, {11, 7, 4, 11, 4, 2, 2, 4, 0, -1, -1, -1, -1, -1, -1, -1}, {11, 7, 4, 11, 4, 2, 8, 3, 4, 3, 2, 4, -1, -1, -1, -1}, {2, 9, 10, 2, 7, 9, 2, 3, 7, 7, 4, 9, -1, -1, -1, -1}, {9, 10, 7, 9, 7, 4, 10, 2, 7, 8, 7, 0, 2, 0, 7, -1}, {3, 7, 10, 3, 10, 2, 7, 4, 10, 1, 10, 0, 4, 0, 10, -1}, {1, 10, 2, 8, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 1, 4, 1, 7, 7, 1, 3, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 1, 4, 1, 7, 0, 8, 1, 8, 7, 1, -1, -1, -1, -1}, {4, 0, 3, 7, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 9, 3, 9, 11, 11, 9, 10, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 10, 0, 10, 8, 8, 10, 11, -1, -1, -1, -1, -1, -1, -1}, {3, 1, 10, 11, 3, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 11, 1, 11, 9, 9, 11, 8, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 9, 3, 9, 11, 1, 2, 9, 2, 11, 9, -1, -1, -1, -1}, {0, 2, 11, 8, 0, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 2, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 8, 2, 8, 10, 10, 8, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 10, 2, 0, 9, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 8, 2, 8, 10, 0, 1, 8, 1, 10, 8, -1, -1, -1, -1}, {1, 10, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 3, 8, 9, 1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 9, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 3, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}}; 对于上面 cubeIndex=8​ 的例子，我们查找triTable[8][] = {3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}，3 表示编号为 3 的边，我们按 3-11-2 的顺序，将各边上的点连接起来。这样，就构成了一个三角形。\n网格分辨率 # 将值已知或可以在空间中任意地方插值的区域多边形化时，我们通常要控制网格的分辨率。可以根据所需的平滑度或可用于显示该表面的算力来生成等值线的粗略近似。下面是用不同网格大小生成的两个\u0026quot;bobby分子\u0026quot;。\n​ ​\n参考 # 原文： http://paulbourke.net/geometry/polygonise/\n原文视频解读： https://www.bilibili.com/video/BV1Ev411r7vx/\n[1] Marching Cubes (carleton.edu)\n[2] https://www.bilibili.com/video/BV1bW4y1675m\n[3] http://paulbourke.net/geometry/polygonise/interp.c\n推荐视频：https://www.bilibili.com/video/BV1yJ411r73v\n‍\n‍\n","date":"29 January 2023","permalink":"/posts/marching-cubes-1/","section":"Posts","summary":"Marching Cubes 详解与实现 # 等值面是空间中的一张曲面，在该曲面上函数F(x，y，z)的值等于某一给定值。准确地讲，是指在某一网格空间中，假若每一结点保存着三变量函数F(x，y，z)，而且网格单元在x，y，z方向上的连续采样值为F(x，y，z)，则对于某一给定值Fi，等值面是由所有满足","title":"Marching Cubes -1 原理介绍"},{"content":" 1. 前言 # 对于人类来说，最初的动作都是在婴儿时期模仿大人的行为习得的。而对机器人来说，想要让它学习人类动作，也需要从模仿开始。其实，平常的机器人示教操作，就是一个初步的模仿过程。我们通过人工示教，来引导机器人做出相应的动作，即技能的学习。\n当今，示教操作主要分为三类，分别是直接动觉示教（Kinematics Teaching）、遥操作示教（Teleoperation）和视频演示示教（Learning from Video）。下面使用机械臂来举例说明：\n直接动觉示教：手动拖动机械臂，让其记忆拖动点位 遥操作示教：使用手持设备或VR技术，远程操作机械臂，记忆过程中的数据流 视频演示示教：通过对视频的特征提取，让机械臂学习相应动作 其中，视频演示示教最为自然，但模仿难度更大，因为它属于间接示教，涉及到复杂的人机解耦问题，是近年来的研究热点，也是本篇文章主要的探讨方向。\n2. 模仿学习发展历程 # 下图是近年来模仿学习方法的演变过程，图片来自 OpenDILab 的分享 https://www.bilibili.com/video/BV1AZ4y1e7WP。图中从上到下是模仿学习方法的主要发展过程，从左到右是某一方法在某一方面的深入。 ​ ​ 行为克隆（Behavior Clone，BC）：指专家做什么，智能体就做一模一样的事，与监督学习较为相似。但问题是，不管专家的行为是否合理或有用，智能体都会完全模仿专家的行为。\n逆强化学习（Inverse RL，IRL）：IRL 与 RL 相比，它没有奖励函数，只有环境和专家示范。在训练的时候，智能体可以与环境交互，但它得不到奖励，它的奖励函数必须从专家示范中反推出来（这也是 IRL 要解决的关键问题）。接下来，再使用 RL 的方法学习出最优演员。\n逆强化学习总体上可以归结为两类：**基于最大边际的逆强化学习**和**基于概率模型的逆强化学习**。 最大边际逆强化学习（Maximum Margin IRL）：在早期，IRL 使用最大边际形式化的思想来反推奖励函数。其缺点是经常有很多不同的奖励函数导致相同的专家策略，在这种情况下，所学到的奖励函数往往具有随机的偏好。所以该方法无法解决歧义的问题，这里就不对该方法进行过多讨论。\n参考文章：\nhttps://zhuanlan.zhihu.com/p/26682811 https://zhuanlan.zhihu.com/p/26766494 https://zhuanlan.zhihu.com/p/30210839 最大熵逆强化学习（Maximum Entropy IRL，MaxEnt IRL）：为了解决最大边际逆强化学习中的多解问题，学术界把目光逐渐转变到基于概率模型的方法上，如最大熵逆强化学习、深度逆强化学习等。\n最大熵原理是指，在学习概率模型时，在所有满足约束的概率模型（分布）中，熵最大的模型是最好的模型。这是因为，通过熵最大所选取的模型，没有对未知分布做任何约束或假设。\n现在，基于现有的理论，我们已经能够生成较好的模型了，但目前仍存在一个问题，即训练的时候需要投入大量的专家示范供模型学习。\n参考文章：\n最大熵逆强化学习： https://zhuanlan.zhihu.com/p/91819689 最大熵逆强化学习实现： https://zhuanlan.zhihu.com/p/95465350 生成对抗模仿学习（Generative Adversarial Imitation Learning，GAIL）：该算法类似于 GAN 与 IRL 的结合，以 Maximum Causal Entropy IRL 作为研究的基础框架，解决了逆强化学习计算成本⾼，学习效率低下的问题。\n我们可以比较一下逆强化学习与生成对抗网络。如下图所示，在生成对抗网络里，我们有一些很好的图、一个生成器和一个判别器。一开始，生成器不知道要产生什么样的图，它就会乱画。判别器的工作是给画的图打分，专家画的图得高分，生成器画的图得低分。生成器会想办法让判别器也给它画的图打高分。整个过程与逆强化学习是一模一样的。专家画的图就是专家示范，生成器就是演员，演员与环境交互产生的轨迹其实就等价于生成器画的这些图。然后我们需要学习的奖励函数就相当于判别器。奖励函数要给专家示范打高分，给演员与环境交互的轨迹打低分。接下来，演员会想办法，从已经学习出的奖励函数中得到高分，然后迭代地循环下去。\n参考文章： 原文地址： https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf https://zhuanlan.zhihu.com/p/354572550 例子源自《Easy-RL》： https://github.com/datawhalechina/easy-rl 生成对抗模仿学习(Wasserstein GAIL，W-GAIL) ​：\n将 Wasserstein 距离 ​与 GAIL 结合，类似于 Wasserstein GAN，实现差异测量，即在训练过程中有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表 GAN 训练得越好，代表生成器产生的图像质量越高。\n参考文章：\nWasserstein GAN 中文： https://zhuanlan.zhihu.com/p/25071913 WGAN 原文： https://arxiv.org/abs/1701.07875 http://multimedia.whu.edu.cn/index.php?a=show\u0026amp;catid=72\u0026amp;id=195\u0026amp;lang=1 3. AI动作捕捉 # 从第一节中，我们总结了视频演示示教是机器模仿领域未来的主要发展方向。由于它属于间接示教，并且我们的专家数据通常来自于第三人称视角，而对于机器人来说，任务是在第一视角下进行的。所以，我们如何把第三人称的专家数据泛化到第一人称，以及对 2D 视频流中 3D 信息的实时特征提取，成为目前研究的主要问题。\n对于第三人称视角模仿学习，我们可以引入**领域对抗训练（domain adversarial training）**的概念。领域对抗训练也是一种生成对抗网络的技术。如图所示，我们希望有一个特征提取器，使两幅不同领域（即不同视角）的图像通过特征提取器后，无法分辨出图像来自哪一个领域（即使智能体在第三人称的时候与它在第一人称时的视角是一样的），就是把最重要的特征提取出来。\n无论使用哪种方法去处理，其实都是基于分层的思想去做的。首先是一个表征学习的过程，我们把视频流中的环境信息通过特征提取器提取出来。之后，再使用提取出的特征信息作为专家示范去学习。\n对于双臂机器人的轨迹规划问题，在这里提出了两个场景。\n场景一：动作捕捉。近年来，元宇宙的概念逐渐被炒得火热，虚拟主播（vtuber）这一行业也正在加速发展，实时动作捕捉技术自然成为虚拟现实领域当下的研究热点之一。其中，实时动捕主要分为两个研究方向：面部动捕与全身动捕，然而我们主要关注全身动捕的一部分——双臂动捕。\n实际上，我们所拍摄的人体动作视频数据，提取出的是人体的关节信息，如何把人体关节信息及其运动轨迹泛化到机器人身上，使机器人在模仿人类动作的同时，保持轨迹的平滑，是一个值得思考的问题。一种方法是，使用学习的方法去优化轨迹。\n场景二：物体交互。物体交互技术是基于动作捕捉的进一步发展，我把它分为三个方面：虚拟交互、现实交互和虚拟现实交互。虚拟交互是完全在虚拟场景中的交互，例如游戏或仿真软件内的人机交互等。现实交互是在真实场景中的交互，如机器人与真实物体的交互。虚拟现实交互是现实的人与虚拟的物品的交互，如 VR 中的力反馈效果。\n当然，这里只讨论双臂与物体的交互。在现实交互中，双臂相较于单臂，有更多的对象参与到交互中，因此具有更高维的动作-状态空间、更高范围的解。目前多数的双臂问题都使用经典的控制方法来解决，但很难用于泛化的环境中。因为在交互中，两臂与物体间会产生摩擦、粘附和变形，这些变化很难明确且精准地表示出来。那么一种有效的方法就是模仿学习，我们为机器人提供人类行为的演示，让机器人学习一个策略去模仿人类动作。\n4. 展望 # 最后，我们可以将动作捕捉与物体交互结合到一起，让机器人学会许多的人类行为。也就是说，我们可以让机器人看大量的视频，令机器人学会视频中的动作，例如跳舞、做家务等等，从而解放大量的底层劳动者，因为模型是可以迁移和泛化的。或者，也可以用于科幻影视作品的摄制过程。但是，目前的主要问题集中在算法稳定性的提升上，将经典的控制算法与机器学习深度融合，提高算法的可解释性，是一个推荐的方向。\n‍\n","date":"29 January 2023","permalink":"/posts/irl-in-double-arm/","section":"Posts","summary":"1.","title":"探讨-协作双臂的动作模仿"},{"content":" Timoshenko Beam # ​elastica.modules​用于构建不同的仿真系统\nimport numpy as np # Import modules from elastica.modules import BaseSystemCollection, Constraints, Forcing, Damping # Import Cosserat Rod Class from elastica.rod.cosserat_rod import CosseratRod # Import Damping Class from elastica.dissipation import AnalyticalLinearDamper # Import Boundary Condition Classes from elastica.boundary_conditions import OneEndFixedRod, FreeRod from elastica.external_forces import EndpointForces # Import Timestepping Functions from elastica.timestepper.symplectic_steppers import PositionVerlet from elastica.timestepper import integrate 在这个例子中，杆的一端被固定住，令另一端受力。\nclass TimoshenkoBeamSimulator(BaseSystemCollection, Constraints, Forcing, Damping): pass timoshenko_sim = TimoshenkoBeamSimulator() 接下来，定义这个杆的各项属性，包括材料、几何形状等。\n# setting up test params # rod 中单元体 elements 的数量 n_elem = 100 density = 1000 nu = 1e-4 E = 1e6 # 弹性模量 # For shear modulus of 1e4, nu is 99! # 泊松比，一般不超过0.5，这里是为了让变形更明显 poisson_ratio = 99 shear_modulus = E / (poisson_ratio + 1.0) # 剪切系数 # 三维空间中的起始坐标 start = np.zeros((3,)) # rod 的朝向 direction = np.array([0.0, 0.0, 1.0]) normal = np.array([0.0, 1.0, 0.0]) base_length = 3.0 base_radius = 0.25 base_area = np.pi * base_radius ** 2 我们根据上述的参数构建出一根 rod，然后把它加入到一开始创建的仿真系统中\nshearable_rod = CosseratRod.straight_rod( n_elem, start, direction, normal, base_length, base_radius, density, 0.0, # internal damping constant, deprecated in v0.3.0 E, shear_modulus=shear_modulus, ) timoshenko_sim.append(shearable_rod) 添加阻尼 # We also need to define damping_constant​ and simulation time_step​ and pass in .using()​ method.\ndl = base_length / n_elem dt = 0.01 * dl timoshenko_sim.dampen(shearable_rod).using( AnalyticalLinearDamper, damping_constant=nu, time_step=dt, ) 添加边界条件 # 第一个约束是，固定杆一端的位置。We do this using the .constrain()option and theOneEndFixedRodboundary condition. We are modifying thetimoshenko_simsimulator toconstraintheshearable_rodobject using theOneEndFixedRod type of constraint. ​​\n我们还要定义杆的哪一个节点需要被约束.，可以通过节点的索引 constrained_position_idx​来实现，这里我们固定了第一个节点。为了防止杆绕固定节点旋转, 我们需要在两个节点之间约束一个元素，这固定了杆的方向. 我们通过约束元素的索引 constrained_director_idx​来实现。例如对于 position, 我们约束杆的第一个元素. Together, this contrains the position and orientation of the rod at the origin.\ntimoshenko_sim.constrain(shearable_rod).using( OneEndFixedRod, constrained_position_idx=(0,), constrained_director_idx=(0,) ) print(\u0026#34;One end of the rod is now fixed in place\u0026#34;) 第二个约束是对杆的末端施加一个力。我们想在d1方向施加一个负的力，同时加到杆的末端，可以通过指定origin_force 和end_force 实现。我们还想随着时间逐渐提高力的大小，通过指定ramp_up_time来改变，这防止了因力的不连续导致的错误。\norigin_force = np.array([0.0, 0.0, 0.0]) end_force = np.array([-10.0, 0.0, 0.0]) ramp_up_time = 5.0 timoshenko_sim.add_forcing_to(shearable_rod).using( EndpointForces, origin_force, end_force, ramp_up_time=ramp_up_time ) print(\u0026#34;Forces added to the rod\u0026#34;) 添加 Unshearable Rod # 为了比较 shearable rod 和 unshearable rod，我们再添加一个unshearable rod。我们改变 rod 的泊松比来让它 unshearable。对于真实的unshearable rod，泊松比通常为 -1.0，然而这会导致系统数值不稳定，所以我们采用 -0.85 的泊松比。\n# Start into the plane unshearable_start = np.array([0.0, -1.0, 0.0]) unshearable_rod = CosseratRod.straight_rod( n_elem, unshearable_start, direction, normal, base_length, base_radius, density, 0.0, # internal damping constant, deprecated in v0.3.0 E, # Unshearable rod needs G -\u0026gt; inf, which is achievable with a poisson ratio of -1.0 shear_modulus=E / (-0.85 + 1.0), ) timoshenko_sim.append(unshearable_rod) timoshenko_sim.dampen(unshearable_rod).using( AnalyticalLinearDamper, damping_constant=nu, time_step=dt, ) timoshenko_sim.constrain(unshearable_rod).using( OneEndFixedRod, constrained_position_idx=(0,), constrained_director_idx=(0,) ) timoshenko_sim.add_forcing_to(unshearable_rod).using( EndpointForces, origin_force, end_force, ramp_up_time=ramp_up_time ) print(\u0026#34;Unshearable rod set up\u0026#34;) 系统结束 # 现在我们以及添加完需要的rods及其边界条件到我们的系统中。最后，我们需要结束这个系统。这个操作将遍历系统，重新排列事物，并预计算有用的数值，为系统进行仿真做好准备。\ntimoshenko_sim.finalize() print(\u0026#34;System finalized\u0026#34;) 注意，如果在 finalize 后对 rod 做出了一些改变，需要 re-setup 该系统，即重新运行上述的所有代码。\n定义仿真时间 # 我们还要决定该仿真的运行时间，以及使用哪种时间步长方法。默认方法是 PositionVerlet 算法。这里我们仿真10s。\nfinal_time = 10.0 total_steps = int(final_time / dt) print(\u0026#34;Total steps to take\u0026#34;, total_steps) timestepper = PositionVerlet() 运行仿真 # 对于仿真的运行，我们结合 timoshenko_sim，使用 timestepper 方法，执行了 total_steps 步后，到达 final_time。\nintegrate(timestepper, timoshenko_sim, final_time, total_steps)\n‍\nPost Processing Results # 现在仿真已经结束，我们想处理仿真结果，我们将比较the solutions for the shearable and unshearable beams​与analytical Timoshenko and Euler-Bernoulli beam results. ​​\n# Compute beam position for sherable and unsherable beams. def analytical_result(arg_rod, arg_end_force, shearing=True, n_elem=500): base_length = np.sum(arg_rod.rest_lengths) # 在间隔 0.0 和 base_length 之间返回 n_elem 个均匀间隔的数据 # 即每个节点距离起始点的长度 arg_s = np.linspace(0.0, base_length, n_elem) if type(arg_end_force) is np.ndarray: acting_force = arg_end_force[np.nonzero(arg_end_force)] else: acting_force = arg_end_force acting_force = np.abs(acting_force) linear_prefactor = -acting_force / arg_rod.shear_matrix[0, 0, 0] quadratic_prefactor = ( -acting_force / 2.0 * np.sum(arg_rod.rest_lengths / arg_rod.bend_matrix[0, 0, 0]) ) cubic_prefactor = (acting_force / 6.0) / arg_rod.bend_matrix[0, 0, 0] if shearing: return ( arg_s, arg_s * linear_prefactor + arg_s ** 2 * quadratic_prefactor + arg_s ** 3 * cubic_prefactor, ) else: return arg_s, arg_s ** 2 * quadratic_prefactor + arg_s ** 3 * cubic_prefactor 现在，我们想去画出结果。首先需要指出的是，如何接收杆的位置，它们位于rod.position_collection[dim, n_elem]​。在本例中，我们画出 x- 和 z-轴。\ndef plot_timoshenko(shearable_rod, unshearable_rod, end_force): import matplotlib.pyplot as plt analytical_shearable_positon = analytical_result( shearable_rod, end_force, shearing=True ) analytical_unshearable_positon = analytical_result( unshearable_rod, end_force, shearing=False ) fig = plt.figure(figsize=(5, 4), frameon=True, dpi=150) ax = fig.add_subplot(111) ax.grid(b=True, which=\u0026#34;major\u0026#34;, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=0.25) ax.plot( analytical_shearable_positon[0], analytical_shearable_positon[1], \u0026#34;k--\u0026#34;, label=\u0026#34;Timoshenko\u0026#34;, ) ax.plot( analytical_unshearable_positon[0], analytical_unshearable_positon[1], \u0026#34;k-.\u0026#34;, label=\u0026#34;Euler-Bernoulli\u0026#34;, ) ax.plot( shearable_rod.position_collection[2, :], shearable_rod.position_collection[0, :], \u0026#34;b-\u0026#34;, label=\u0026#34;n=\u0026#34; + str(shearable_rod.n_elems), ) ax.plot( unshearable_rod.position_collection[2, :], unshearable_rod.position_collection[0, :], \u0026#34;r-\u0026#34;, label=\u0026#34;n=\u0026#34; + str(unshearable_rod.n_elems), ) ax.legend(prop={\u0026#34;size\u0026#34;: 12}) ax.set_ylabel(\u0026#34;Y Position (m)\u0026#34;, fontsize=12) ax.set_xlabel(\u0026#34;X Position (m)\u0026#34;, fontsize=12) plt.show() plot_timoshenko(shearable_rod, unshearable_rod, end_force) ​ ‍​ ","date":"28 January 2023","permalink":"/posts/timoshenko-beam/","section":"Posts","summary":"Timoshenko Beam # ​elastica.","title":"使用 pyelastic 对悬臂梁进行仿真"},{"content":"","date":"13 June 2022","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Dummy Second Author\u0026rsquo;s awesome dummy bio.\n","date":"1 January 0001","permalink":"/authors/zhou/","section":"Authors","summary":"Dummy Second Author\u0026rsquo;s awesome dummy bio.","title":"海豚"}]